# -*- coding: utf-8 -*-
"""01_register_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15varczMUrG9sE4wz2vnBfavbNThdGCrR
"""

# -*- coding: utf-8 -*-
"""01_register_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15varczMUrG9sE4wz2vnBfavbNThdGCrR
"""

import os, json
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import joblib

from huggingface_hub import HfApi, hf_hub_download

HF_DATASET_REPO = os.environ["HF_DATASET_REPO"]
HF_MODEL_REPO   = os.environ["HF_MODEL_REPO"]
SEED = int(os.environ.get("SEED", "42"))

REPORT_DIR = "reports"
MODEL_DIR = "models"
os.makedirs(REPORT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

def load_split(filename: str) -> pd.DataFrame:
    path = hf_hub_download(
        repo_id=HF_DATASET_REPO,
        repo_type="dataset",
        filename=f"data/processed/{filename}",
    )
    return pd.read_csv(path)

def build_pipeline(X: pd.DataFrame):
    cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
    num_cols = [c for c in X.columns if c not in cat_cols]

    numeric = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median"))
    ])
    categorical = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    pre = ColumnTransformer(
        transformers=[
            ("num", numeric, num_cols),
            ("cat", categorical, cat_cols),
        ]
    )

    clf = RandomForestClassifier(
        random_state=SEED,
        n_jobs=-1,
        class_weight="balanced"
    )

    pipe = Pipeline(steps=[
        ("preprocess", pre),
        ("model", clf)
    ])
    return pipe

def main():
    train_df = load_split("train.csv")
    test_df  = load_split("test.csv")


    y_train = train_df["ProdTaken"].astype(int)
    X_train = train_df.drop(columns=["ProdTaken", "CustomerID"], errors="ignore")

    y_test = test_df["ProdTaken"].astype(int)
    X_test = test_df.drop(columns=["ProdTaken", "CustomerID"], errors="ignore")

    pipe = build_pipeline(X_train)

    param_grid = {
        "model__n_estimators": [200, 400],# -*- coding: utf-8 -*-
"""01_register_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15varczMUrG9sE4wz2vnBfavbNThdGCrR
"""

import os
import pandas as pd
import streamlit as st
import joblib
from huggingface_hub import hf_hub_download

HF_MODEL_REPO = os.environ.get("HF_MODEL_REPO", "")
MODEL_FILE = "best_model.joblib"

st.set_page_config(page_title="Visit with Us – Wellness Package Predictor", layout="centered")
st.title("Wellness Tourism Package – Purchase Prediction")

@st.cache_resource
def load_model():
    model_path = hf_hub_download(repo_id=HF_MODEL_REPO, repo_type="model", filename=MODEL_FILE)
    return joblib.load(model_path)

model = load_model()

st.subheader("Enter customer details")

# Keep fields aligned with your dataset columns (except ProdTaken)
age = st.number_input("Age", min_value=18, max_value=100, value=35)
typeofcontact = st.selectbox("TypeofContact", ["Company Invited", "Self Enquiry"])
citytier = st.selectbox("CityTier", [1, 2, 3])
duration = st.number_input("DurationOfPitch", min_value=0.0, value=10.0)
occupation = st.selectbox("Occupation", ["Salaried", "Free Lancer", "Small Business", "Large Business"])
gender = st.selectbox("Gender", ["Male", "Female"])
nop = st.number_input("NumberOfPersonVisiting", min_value=1, value=2)
followups = st.number_input("NumberOfFollowups", min_value=0.0, value=3.0)
productpitched = st.selectbox("ProductPitched", ["Basic", "Deluxe", "Standard", "Super Deluxe", "King"])
star = st.number_input("PreferredPropertyStar", min_value=1.0, max_value=5.0, value=3.0)
marital = st.selectbox("MaritalStatus", ["Single", "Married", "Divorced"])
trips = st.number_input("NumberOfTrips", min_value=0.0, value=2.0)
passport = st.selectbox("Passport", [0, 1])
pitchscore = st.selectbox("PitchSatisfactionScore", [1, 2, 3, 4, 5])
owncar = st.selectbox("OwnCar", [0, 1])
kids = st.number_input("NumberOfChildrenVisiting", min_value=0.0, value=0.0)
designation = st.selectbox("Designation", ["Executive", "Manager", "Senior Manager", "AVP", "VP"])
income = st.number_input("MonthlyIncome", min_value=0.0, value=25000.0)

row = {
    "Age": age,
    "TypeofContact": typeofcontact,
    "CityTier": citytier,
    "DurationOfPitch": duration,
    "Occupation": occupation,
    "Gender": gender,
    "NumberOfPersonVisiting": nop,
    "NumberOfFollowups": followups,
    "ProductPitched": productpitched,
    "PreferredPropertyStar": star,
    "MaritalStatus": marital,
    "NumberOfTrips": trips,
    "Passport": passport,
    "PitchSatisfactionScore": pitchscore,
    "OwnCar": owncar,
    "NumberOfChildrenVisiting": kids,
    "Designation": designation,
    "MonthlyIncome": income,
}

if st.button("Predict"):
    X = pd.DataFrame([row])
    if "CustomerID" not in X.columns:
      X["CustomerID"] = 0   # dummy value
    proba = model.predict_proba(X)[:, 1][0]
    pred = int(proba >= 0.5)
    st.write("### Result")
    st.metric("Purchase Probability", f"{proba:.2%}")
    st.success("Likely to Purchase ✅" if pred == 1 else "Unlikely to Purchase ❌")
    st.caption("Tip: use probability for targeting thresholds (e.g., >0.65 = priority lead).")
        "model__max_depth": [None, 8, 16],
        "model__min_samples_split": [2, 10],
        "model__min_samples_leaf": [1, 5],
        "model__max_features": ["sqrt", "log2"],
    }

    gs = GridSearchCV(
        estimator=pipe,
        param_grid=param_grid,
        scoring="roc_auc",
        cv=5,
        n_jobs=-1,
        verbose=1
    )

    gs.fit(X_train, y_train)
    best = gs.best_estimator_

    # Evaluate
    proba = best.predict_proba(X_test)[:, 1]
    pred  = (proba >= 0.5).astype(int)

    metrics = {
        "roc_auc": float(roc_auc_score(y_test, proba)),
        "f1": float(f1_score(y_test, pred)),
        "accuracy": float(accuracy_score(y_test, pred)),
        "best_params": gs.best_params_,
    }

    # Save artifacts locally
    model_path = f"{MODEL_DIR}/best_model.joblib"
    joblib.dump(best, model_path)

    with open(f"{REPORT_DIR}/metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)

    with open(f"{REPORT_DIR}/classification_report.txt", "w") as f:
        f.write(classification_report(y_test, pred))

    print("✅ Metrics:", metrics)

    # Register model on Hugging Face Model Hub
    api = HfApi()
    api.create_repo(repo_id=HF_MODEL_REPO, repo_type="model", exist_ok=True)

    api.upload_file(
    path_or_fileobj=model_path,
    path_in_repo="best_model.joblib",
    repo_id=HF_MODEL_REPO,
    repo_type="model",
)
    api.upload_file(
    path_or_fileobj=f"{REPORT_DIR}/metrics.json",
    path_in_repo="metrics.json",
    repo_id=HF_MODEL_REPO,
    repo_type="model",
)
    api.upload_file(
    path_or_fileobj=f"{REPORT_DIR}/classification_report.txt",
    path_in_repo="classification_report.txt",
    repo_id=HF_MODEL_REPO,
    repo_type="model",
)


    print("✅ Uploaded model + metrics to HF model repo:", HF_MODEL_REPO)

if __name__ == "__main__":
    main()